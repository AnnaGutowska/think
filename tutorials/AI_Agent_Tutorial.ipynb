{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a LangChain AI Agent in Python with watsonx \n",
    "\n",
    "**Author**: Anna Gutowska \n",
    "\n",
    "In this tutorial, we will use the LangChain Python package to build an AI agent that uses its custom tools to return a URL directing to [NASA's Astronomy Picture of the Day](https://apod.nasa.gov/apod/astropix.html).\n",
    "\n",
    "An [artificial intelligence (AI) agent](https://www.ibm.com/think/topics/ai-agents?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Create+a+LangChain+AI+Agent+in+Python+with+watsonx_v1_1720547929) is a system that performs tasks on behalf of a user or another system by designing its own workflow and utilizing available tools.\n",
    "\n",
    "# Overview of AI agents \n",
    "One of the most common modalities of agentic AI approaches is chatbots. However, agentic technology can encompass a wide range of functions. These include planning, problem-solving, interacting with external environments and executing actions. These agents can be deployed to solve complex tasks in various enterprise contexts. From software design and IT automation to code-generation tools and conversational AI assistants, AI agents leverage the capability of [Large Language Models (LLMs)](https://www.ibm.com/topics/large-language-models) to work step-by-step. For this reason, they are otherwise known as LLM agents.\n",
    "\n",
    "Key processes that make AI agents unique in their autonomy are: \n",
    "\n",
    "* **Goal initialization and planning**. Although AI agents are autonomous in their planning of future actions, they require goals and environments defined by humans. For simple tasks, planning is not a necessary step. Instead, an agent can iteratively reflect on its responses and improve them without planning its next steps.\n",
    "\n",
    "* **Reasoning using available tools**. An AI agent’s plan of action is based on the information it perceives. Often, AI agents do not have the full knowledge base that is needed for tackling all subtasks within a complex goal. To remedy this, AI agents use their available tools. These tools can include external datasets, algorithms, search tools, APIs and even other agents. We can instruct agents to \"think\" slowly, plan ahead and display each \"thought\" by aligning the prompt structure with a ReAct (Reasoning and Action) framework. These loops of thinking, acting and responding are used to solve problems step by step. The verbal reasoning produced gives insight into how responses are being formulated. \n",
    "\n",
    "* **Learning and reflection**. AI agents use feedback mechanisms, such as other AI agents and human-in-the-loop (HITL), to improve the accuracy of their responses.\n",
    "\n",
    "Traditional LLMs, like the IBM® [Granite™ models](https://www.ibm.com/products/watsonx-ai/foundation-models?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Create+a+LangChain+AI+Agent+in+Python+with+watsonx_v1_1720547929) that we will be using in this tutorial, are limited in their knowledge and reasoning. They produce their responses based on the data used to train them, which can often include out-of-date information. In contrast, agentic technology uses tool-calling on the backend to obtain up-to-date information, optimize workflow and create specific tasks autonomously to achieve complex goals. In this process, the autonomous agent is learning to adapt to user expectations over time, providing a  personalized experience and comprehensive responses. This tool-calling can be achieved without human intervention and broadens the possibilities for real-world applications of these AI systems.\n",
    "\n",
    "We encourage you to check out our [AI Agents](https://www.ibm.com/think/topics/ai-agents?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Create+a+LangChain+AI+Agent+in+Python+with+watsonx_v1_1720816665) article for more in-depth information on the various AI agent types and their abstractions.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "You need an [IBM Cloud account](https://cloud.ibm.com/registration?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial) to create a [watsonx.ai](https://www.ibm.com/products/watsonx-ai?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-product) project.\n",
    "\n",
    "# Steps\n",
    "\n",
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. Jupyter Notebooks are widely used within [data science](https://www.ibm.com/topics/data-science?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom) to combine code, text, images, and [data visualizations](https://www.ibm.com/topics/data-visualization?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom) to formulate a well-formed analysis.\n",
    "\n",
    "1. Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial) using your IBM Cloud account.\n",
    "\n",
    "2. Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&topic=projects-creating-project&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom).\n",
    "\n",
    "\tTake note of the project ID in project > Manage > General > Project ID. You’ll need this ID for this tutorial.\n",
    "\n",
    "3. Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&topic=editor-creating-managing-notebooks&cm_sp=ibmdev-_-developer-tutorials-_-ibmcom).\n",
    "\n",
    "This step will open a Notebook environment where you can copy the code from this tutorial to implement an AI agent of your own. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This Jupyter Notebook is available on [GitHub](https://github.com/AnnaGutowska/think/blob/main/tutorials/AI_Agent_Tutorial.ipynb).\n",
    "\n",
    "## Step 2. Set up a Watson Machine Learning service instance and API key\n",
    "\n",
    "1. Create a [Watson Machine Learning](https://cloud.ibm.com/catalog/services/watson-machine-learning?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-tutorials-_-trial) service instance (choose the Lite plan, which is a free instance).\n",
    "\n",
    "2. Generate an [API Key in WML](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&context=cpdaas). \n",
    "\n",
    "\n",
    "3. Associate the WML service to the project you created in [watsonx.ai](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&context=cpdaas). \n",
    "\n",
    "\n",
    "## Step 3. Install and import relevant libraries and set up your credentials\n",
    "\n",
    "We'll need a few libraries and modules for this tutorial. Make sure to import the ones below, and if they're not installed, you can resolve this with a quick pip install. LangChain will be the framework and developer toolkit used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installations\n",
    "%pip install langchain\n",
    "%pip install langchain_ibm\n",
    "%pip install langchain_core\n",
    "%pip install IPython\n",
    "%pip install nasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nasapy\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your credentials. Input your API key and project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": \"YOUR_API_KEY_HERE\",   \n",
    "    \"project_id\": \"YOUR_PROJECT_ID_HERE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's establish our connection with the NASA API that we will be using later in this activity. This API does not require authentication. With a small rate limit, you can get started using 'DEMO_KEY' as your key. To avoid the low rate limitations, you can [register for your own NASA API key](https://api.nasa.gov/) and replace it as the key value below. Registering for a personal key is quick, free, and simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nasapy.Nasa(key='DEMO_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Initialize a basic agent with no tools\n",
    "\n",
    "This step is important as it will produce a clear example of an agent's behavior with and without external data sources. Let's start by setting our parameters.\n",
    "\n",
    "The model parameters available can be found [here](https://ibm.github.io/watson-machine-learning-sdk/model.html). We experimented with various model parameters, including temperature, minimum and maximum new tokens and stop sequences. Learn more about model parameters and what they mean in the [watsonx docs](https://www.ibm.com/docs/en/watsonx/saas?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Create+a+LangChain+AI+Agent+in+Python+with+watsonx_v1_1721141990&topic=lab-model-parameters-prompting). It is important to set our `stop_sequences` here in order to limit agent hallucinations. This tells the agent to stop producing further output upon encountering particular substrings. In our case, we want the agent to end its response upon reaching an observation. Hence, one of our `stop_sequences` is `'\\nObservation'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"temperature\": 0, \n",
    "    \"min_new_tokens\": 5,\n",
    "    \"max_new_tokens\": 250,\n",
    "    \"stop_sequences\":['\\nObservation', '\\n\\n']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we suggest using IBM's Granite 13B Chat model as the LLM to achieve similar results. You are free to use any AI model of your choice. The foundation models available through watsonx can be found [here](https://www.ibm.com/products/watsonx-ai/foundation-models?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Create+a+LangChain+AI+Agent+in+Python+with+watsonx_v1_1720816665). The purpose of these models in LLM applications is to serve as the reasoning engine that decides which actions to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WatsonxLLM(\n",
    "\tmodel_id =  \"ibm/granite-13b-chat-v2\",\n",
    "\turl = credentials.get(\"url\"),\n",
    "\tapikey = credentials.get(\"apikey\"),\n",
    "\tproject_id =  credentials.get(\"project_id\"),\n",
    "\tparams = param\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a prompt template in case you want to ask multiple questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the {query} accurately.\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can set up a chain with our prompt and our LLM. This allows the generative model to produce a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step of this tutorial, we will be creating a tool that retrieves today's date. As we have covered, traditional LLMs cannot obtain the current date on their own. Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nA: Today is [display current date].\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"query\": \"What is today's date?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the LLM is unable to provide us with the current date. The training data used for this model contained information prior to today's date and without the appropriate tools, the agent does not have access to real-time information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Define the agent's tools\n",
    "\n",
    "Unlike traditional LLMs, AI agents can provide more comprehensive responses to diverse tasks through their tool usage, memory, and planning. Let's define the two tools our agent will be using: \n",
    "\n",
    "* `get_todays_date()` - uses the Python `datetime` package to return today's date in YYYY-MM-DD format. \n",
    "* `get_astronomy_image()` - utilizes the NASA API to obtain the Astronomy Picture of the Day. After the tool acquires the image, its URL is returned. \n",
    "\n",
    "Note: The date variable is set to an empty string because it is used by both tools and is thus, a global variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = ''   \n",
    "\n",
    "@tool\n",
    "def get_todays_date():\n",
    "    \"\"\"Get today's date in YYYY-MM-DD format.\"\"\"\n",
    "    global date\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    return date\n",
    "\n",
    "\n",
    "@tool(return_direct=True)\n",
    "def get_astronomy_image():\n",
    "    \"\"\"Get NASA's Astronomy Picture of the Day on given date.\"\"\"\n",
    "    global date\n",
    "        \n",
    "    apod = n.picture_of_the_day(date, hd=True)\n",
    "    \n",
    "    return apod['url']\n",
    "\n",
    "\n",
    "tools = [get_todays_date, get_astronomy_image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Establish the prompt template\n",
    "\n",
    "Next, we will set up a new prompt template to ask multiple questions. This template is more complex. It is refered to as a [structured chat prompt](https://api.python.langchain.com/en/latest/agents/langchain.agents.structured_chat.base.create_structured_chat_agent.html#langchain-agents-structured-chat-base-create-structured-chat-agent) and is used for creating agents that have multiple tools available. It will be made up of a `system_prompt`, a `human_prompt` and the tools we defined in Step 5. \n",
    "\n",
    "First, we will set up the `system_prompt`. This prompt instructs the agent to print its \"thought process,\" which involves the agent's subtasks, the tools that were used and the final output. This gives us insight into the agent's function calling. The prompt also instructs the agent to return its responses in JSON Blob format and to consider information it has stored in its memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\n",
    "\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "Follow this format:\n",
    "\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we are establishing the `human_prompt`. This prompt tells the agent to display the user input followed by the intermediate steps taken by the agent as part of the `agent_scratchpad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = \"\"\"{input}\n",
    "{agent_scratchpad}\n",
    "(reminder to always respond in a JSON blob)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we establish the order of our newly defined prompts. We create this new template to feature the `system_prompt` followed by an optional list of messages collected in the agent's memory, if any, and finally, the `human_prompt` which includes both the human input and `agent_scratchpad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets finalize our prompt template by adding the tool names, descriptions and arguments using a [partial prompt template](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/partial/). This allows the agent to access the information pertaining to each tool including the intended use cases. This also means we can add and remove tools without altering our entire prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt.partial(\n",
    "    tools=render_text_description_and_args(list(tools)),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Set up the agent's memory and chain\n",
    "\n",
    "An important feature of AI agents is their memory. Agents are able to store past conversations and past findings in their memory to improve the accuracy and relevance of their responses going forward. In our case, we will use LangChain's `ConversationBufferMemory()` as a means of memory storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can set up a chain with our agent's scratchpad, memory, prompt and the LLM. The `AgentExecutor` class is used to execute the agent. It takes the agent, its tools, error handling approach, verbose parameter and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ( RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        chat_history=lambda x: memory.chat_memory.messages,\n",
    "    )\n",
    "    | prompt | model | JSONAgentOutputParser()                                               \n",
    ")                           \n",
    "\n",
    "agent_executor = AgentExecutor(agent=chain, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Generate responses with the AI agent\n",
    "\n",
    "We are now able to ask the agent questions. Recall the agent's previous inability to provide us with the current date. Now that the agent has its tools available to use, let's try asking the same question again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_todays_date\",\n",
      "  \"action_input\": {}\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2024-07-15\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Today is 2024-07-15.\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What is today's date?\",\n",
       " 'history': '',\n",
       " 'output': 'Today is 2024-07-15.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is today's date?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The agent is now able to tell us the current date. As seen in the JSON Blob produced, the agent recognized that it should utilize the `get_todays_date` tool to reach this result. \n",
    "\n",
    "Now, let's try asking a more complex question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_todays_date\",\n",
      "  \"action_input\": {}\n",
      "}\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2024-07-15\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_astronomy_image\",\n",
      "  \"action_input\": {}\n",
      "}\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mhttps://apod.nasa.gov/apod/image/2407/TadpoleGalaxy_HubblePathak_960.jpg\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Show me NASA's Astronomy Picture of the Day today.\",\n",
       " 'history': \"Human: What is today's date?\\nAI: Today is 2024-07-15.\",\n",
       " 'output': 'https://apod.nasa.gov/apod/image/2407/TadpoleGalaxy_HubblePathak_960.jpg'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Show me NASA's Astronomy Picture of the Day today.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! The agent used both of its available tools to return a URL that leads to an image of today's Astronomy Picture of the Day via NASA's API. Because we wanted to see the image from today, the agent used the `get_todays_date` tool first in order to then use that date for the `get_astronomy_image` tool. Additionally, the agent is successfully updating its knowledge base as it learns new information and experiences new interactions as seen by the history output.\n",
    "\n",
    "To check out the image, click the URL your agent produces or copy and paste it into a browser. Please note that your agent will generate a different link than the one shown above since the dates will differ.\n",
    "\n",
    "And that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you created an AI agent using LangChain in Python with watsonx. You created a tool to return today's date and another tool to return today's Astronomy Picture of the Day using NASA's open-source API.\n",
    "\n",
    "The sample output is important as it shows the steps the agent took in creating its own agent workflow using available tools. In our case, the LLM on its own was not able to achieve the first subtask of the problem: finding the current date. Hence, the tools granted to the agent were vital for achieving the goal.\n",
    "\n",
    "We encourage you to check out the [LangChain documentation page](https://python.langchain.com/v0.2/docs/tutorials/agents/) for more information and tutorials on AI agents.\n",
    "\n",
    "\n",
    "## Try watsonx for free\n",
    "\n",
    "Build an AI strategy for your business on one collaborative AI and data platform called IBM [watsonx](https://www.ibm.com/watsonx?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-product), which brings together new generative AI capabilities, powered by foundation models, and traditional machine learning into a powerful platform spanning the AI lifecycle. With [watsonx.ai](https://www.ibm.com/products/watsonx-ai?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-product), you can train, validate, tune, and deploy models with ease and build AI applications in a fraction of the time with a fraction of the data.\n",
    "\n",
    "Try [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial), the next-generation studio for AI builders.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Explore [more articles and tutorials about watsonx](https://developer.ibm.com/components/watsonx/?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx) on IBM Developer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
